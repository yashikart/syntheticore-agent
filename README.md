# ğŸ§  SYNTHETICORE-AGENT

**SynthetiCore Agent ** is a smart, feedback-driven synthetic data generation framework designed to create realistic, bias-aware datasets across key domains such as **Finance**, **Education**, and **Health**. It combines **structured simulation**, **LLM-enhanced text generation**, and **reinforcement learning (RL)** to dynamically improve the quality and realism of generated data based on user feedback.

> ğŸ” Built to simulate training data for environments like **Uniguru**, **BH Core**, **Financial Sim**, and **Gurukul**.

---

## ğŸŒŸ Key Features

- ğŸ› **Multi-domain simulation** (Finance, Education, Health)
- ğŸ” **RL-based configuration selection** using reward feedback
- ğŸ§  **GPT-powered summaries**, labels, and natural descriptions
- ğŸŒ **Language support**: English, Hindi, Hinglish (GPT translation)
- ğŸ§ª Realism toggle: `Synthetic` vs `Grounded` (LLM-enhanced)
- ğŸ§° Export: CSV, JSON
- ğŸ™ï¸ Optional: Voice-based prompt input using mic
- ğŸ“ˆ Reward & feedback logging

---

## ğŸ“¦ Project Structure

```
SYNTHETICORE-AGENTAI/
â”‚
â”œâ”€â”€ app.py                     # Main Streamlit app UI
â”œâ”€â”€ rl_reward_log.csv          # RL reward tracking log
â”œâ”€â”€ feedback_log.csv           # User feedback + sentiment log
â”‚
â”œâ”€â”€ data_generators/           # Domain-wise data generators
â”‚   â”œâ”€â”€ finance_generator.py
â”‚   â”œâ”€â”€ education_generator.py
â”‚   â”œâ”€â”€ health_generator.py
â”‚   â””â”€â”€ custom_data_generator.py    # Prompt-based generator
â”‚
â”œâ”€â”€ training_rl/               # PPO-based training modules
â”‚   â”œâ”€â”€ train_multi_domain.py
â”‚   â”œâ”€â”€ multi_domain_env.py
â”‚   â””â”€â”€ multi_domain_inference.py
â”‚
â”œâ”€â”€ utils/                     # Supporting logic
â”‚   â”œâ”€â”€ rl_agent.py                 # Rule-based RL agent (non-ppo)
â”‚   â”œâ”€â”€ reward_functions.py        # Reward logic per domain
â”‚   â”œâ”€â”€ llm_generator.py           # GPT-based summary generator
â”‚   â””â”€â”€ hindi_translator.py        # GPT-based row translation
```

---

## ğŸ–¥ï¸ UI Features (`app.py`)

- ğŸ› Select dataset domain (Finance / Education / Health)
- ğŸ§  Toggle realism (Synthetic / GPT-grounded)
- ğŸŒ Select output language (English / Hindi / Hinglish)
- âš–ï¸ Adjust bias via slider
- ğŸ” Preview data
- ğŸ“¥ Export as `.csv` or `.json`
- ğŸ’¬ Submit feedback for each dataset
- ğŸ“ˆ Visualize average reward trend over time

---

## ğŸ¤– Reinforcement Learning Logic

The **RL Agent** uses one of four preset configurations:

```python
configs = [
    {"realism": "Synthetic", "bias": 0},
    {"realism": "Synthetic", "bias": 20},
    {"realism": "Grounded", "bias": 10},
    {"realism": "Grounded", "bias": 30}
]
```

Each time a dataset is generated:

- The agent selects the configuration with the **highest cumulative reward**.
- Reward is computed using:

```python
reward = realism_score + balance_score + variability_score
```

### ğŸ§¾ Example â€“ Finance Reward Breakdown

- **Realism**: `Income > Expense`
- **Balance**: `Label âˆˆ {"Good Saver", "Over-Spender"}`
- **Variability**: Diversity in `Occupation`, `Region`, and `Behavior`

Similar reward logic is applied for **Education** and **Health** domains.  
User feedback is analyzed using **GPT-3.5**, which extracts:

- A **rating** (1â€“5)
- **Sentiment**: Positive / Negative

These are used to update the RL agentâ€™s reward values dynamically.

---

## ğŸ§ª Domain Modules

Each domain has a dedicated generator function:

### ğŸ¦ Finance

```python
generate_finance_data(realism="Grounded", bias=30)
```

- **Fields**: Income, Expense, Goals, Occupation, Label  
- **Summary**: GPT-generated financial profile

---

### ğŸ“ Education

```python
generate_education_data(realism="Grounded", bias=20)
```

- **Fields**: Strengths, Weaknesses, Progress  
- **Summary**: GPT-generated feedback & learning strategy

---

### ğŸ¥ Health

```python
generate_health_data(realism="Grounded", bias=10)
```

- **Fields**: Diagnosis, Symptoms, Severity  
- **Summary**: GPT-generated medical case report

> âœ… All domain datasets can be optionally **translated to Hindi or Hinglish** using GPT-powered translation.

---

## âœ¨ LLM Usage

- ğŸ“ **Summarization**: GPT generates summaries per row or profile
- ğŸŒ **Translation**: Converts English rows to Hindi/Hinglish
- ğŸ“Š **Rating Extraction**: GPT interprets natural feedback to extract numeric scores and sentiment
- ğŸ§¾ **Custom Prompt-to-Columns**: Schema inferred from user prompts

---

## ğŸ§  PPO Agent (Optional)

To enable policy optimization using `stable-baselines3`:

### ğŸ‹ï¸ Train Agent

```bash
cd training_rl
python train_multi_domain.py
```

### ğŸ§  Run Inference

```bash
python multi_domain_inference.py
```

> Trained model is saved as `ppo_multi_agent.zip`

---

## ğŸ“¤ Output Examples

- Download formats: `.csv`, `.json`
- Varying columns depending on domain
- Each row includes timestamp, label, summary, and optional translations

---

## ğŸ“Š Reward Tracking

- Logged to `rl_reward_log.csv`
- Fields: `timestamp`, `domain`, `file_name`, `avg_reward`, `record_count`
- Visualized using Altair chart in UI

---

## ğŸ’¬ Feedback Logging

- Logged to `feedback_log.csv`
- Fields: `timestamp`, `domain`, `file_name`, `user_comment`, `GPT_rating`, `sentiment`

---

## ğŸ§‘â€ğŸ“ Persona-Driven Simulation

- **ğŸ“˜ Students**: progress, learning style, goal, feedback
- **ğŸ¥ Patients**: symptoms, diagnosis, severity, lifestyle
- **ğŸ’° Finance Users**: savings goal, spending pattern, occupation

---

## âœ… Deliverables Recap

| Deliverable                          | Status |
| ----------------------------------- | ------ |
| UI with toggles and filters         | âœ…     |
| Multi-domain data generators        | âœ…     |
| RL reward logic                     | âœ…     |
| PPO agent (optional)                | âœ…     |
| Feedback + sentiment loop           | âœ…     |
| Export (CSV/JSON)                   | âœ…     |
| LLM integration (text + translation)| âœ…     |
| RL reward visualizations            | âœ…     |
| Voice prompt via mic_recorder       | âœ…     |

---

## ğŸ“¥ Installation

```bash
git clone https://github.com/yourusername/syntheticore-agentai.git
cd syntheticore-agentai
pip install -r requirements.txt
```

Set your OpenAI API Key:

```bash
export OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
```

Launch the app:

```bash
streamlit run app.py
```

---

## ğŸ“š Learning Goals Covered

- Prompt engineering
- Data realism & bias simulation
- RL in data pipelines
- GPT integration in structured workflows
- Feedback loop using LLM
- Persona-driven simulation

---

## ğŸ‘©â€ğŸ’» Author

**Yashika Tirkey**  
ğŸ“ Passionate about AI Agents, Data Simulation & Applied LLMs

---

## ğŸ Final Notes

- âœ… Modular architecture for plug-and-play domains
- ğŸš€ Easy to extend: Add `mental_health_generator.py`, `ecommerce_generator.py`, etc.
- ğŸ’¬ Optional: Enable Whisper or browser mic for prompt-to-data generation
